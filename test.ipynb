{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета\n",
    "def getting_dataset() -> None:\n",
    "    '''The result of executing this function is a dataset downloaded into the directory \"Downloads\"'''\n",
    "\n",
    "    # Импорт библиотек\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from urllib.parse import urljoin\n",
    "    import time\n",
    "\n",
    "    # Инициализируем WebDriver:\n",
    "    USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 YaBrowser/24.1.0.0 Safari/537.36'\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(f'user-agent={USER_AGENT}')\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    main_url = 'https://www.kaggle.com'\n",
    "    sign_in_url = 'https://www.kaggle.com/account/login'\n",
    "    # Датасет находится на странице: https://www.kaggle.com/datasets/chaitanyakck/medical-text/data\n",
    "    dataset_url = \"https://www.kaggle.com/datasets/chaitanyakck/medical-text/data\"\n",
    "\n",
    "    try:\n",
    "        # Перейдём на страницу входа в аккаунт Kaggle для авторизированного скачивания датасета\n",
    "        driver.get(sign_in_url)\n",
    "\n",
    "        # Нажмём на кнопку \"Sign in with Google\" для авторизации (подразумевая, что аккаунт уже зарегистрирован)\n",
    "        google_sign_in_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//button[contains(., \"Sign in with Google\")]'))\n",
    "            )        \n",
    "        google_sign_in_button.click()\n",
    "\n",
    "        # Добавим ожидание с целью снижения нагрузки на сервер.\n",
    "        time.sleep(3)\n",
    "\n",
    "        try:\n",
    "            # Проверим, что мы вошли в аккаунт и можем скачать датасет под своим аккаунтом.\n",
    "            if driver.find_element((By.XPATH, '//h1[contains(., \"Welcome\")]')):\n",
    "\n",
    "                # Теперь мы вошли в систему и можем переходить к дальнейшим действиям.\n",
    "                # Откроем веб-страницу с датасетом:\n",
    "                driver.get(dataset_url)\n",
    "\n",
    "                # Найдём кнопку \"Download\":\n",
    "                download_button = driver.find_element(By.XPATH, '//button[contains(., \"file_download\")]')\n",
    "                # Если кнопка найдена, нажмём на неё\n",
    "                if download_button:\n",
    "                    download_button.click()\n",
    "                # В противном случае воспользуемся альтернативным способом получения датасета - \n",
    "                # непосредственным ереходом по ссылке загрузки датасета\n",
    "                else:\n",
    "                    # найдём элемент, в котором содержится относительная ссылка\n",
    "                    href_element = driver.find_element(By.XPATH, '//div[@class=\"sc-emfenM sc-fnpAPw cvuSKw gzjyQr\"]/a')\n",
    "                    # извлечём относительную ссылку\n",
    "                    rel_link = href_element.get_attribute('href')\n",
    "                    # составим абсолютный путь на скачивание архива\n",
    "                    ds_download_link = urljoin(main_url, rel_link)\n",
    "                    # перейдём по прямой ссылке загрузки\n",
    "                    driver.get(ds_download_link)\n",
    "\n",
    "\n",
    "                # Можно использовать аналогичный код:\n",
    "                # В этом варианте кода заменён time.sleep() на явные ожидания WebDriverWait(), которые, возможно, являются более надежными.\n",
    "                \n",
    "                # try:\n",
    "                #     download_button = WebDriverWait(driver, 10).until(\n",
    "                #         EC.element_to_be_clickable((By.XPATH, '//button[contains(., \"file_download\")]'))\n",
    "                #     )\n",
    "                #     download_button.click()\n",
    "                # except Exception as e:\n",
    "                #     href_element = WebDriverWait(driver, 10).until(\n",
    "                #         EC.presence_of_element_located((By.XPATH, '//div[@class=\"sc-emfenM sc-fnpAPw cvuSKw gzjyQr\"]/a'))\n",
    "                #     )\n",
    "                #     rel_link = href_element.get_attribute('href')\n",
    "                #     ds_download_link = urljoin(main_url, rel_link)\n",
    "                #     driver.get(ds_download_link)\n",
    "\n",
    "                # WebDriverWait(driver, 10).until(\n",
    "                #     EC.presence_of_element_located((By.CLASS_NAME, \"download-modal\"))\n",
    "                # )\n",
    "\n",
    "                # Также, можно добавить контекстный менеджер with для инициализации и автоматического закрытия WebDriver после завершения работы функции.\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Произошла ошибка в процессе поиска элемента на странице - {e}')\n",
    "        # Подождём, пока загрузится файл:\n",
    "        # Используем ожидание появления элемента с определенным классом, указывающим на загрузку\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"download-modal\"))\n",
    "        )\n",
    "    except Exception as E:\n",
    "        print(f'Произошла ошибка в процессе авторизации - {E}')\n",
    "    # Закроем браузер в любом случае\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     getting_dataset()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разархивирование загруженного архива в рабочую директорию\n",
    "def unzip_and_replace_dataset(zip_path: str =\"C:\\\\Users\\\\Allen\\\\Downloads\\\\archive.zip\", \n",
    "                              extract_to: str = \"D:\\\\GeekBrains\\\\Data_engineer_Diploma_project\") -> None:\n",
    "    '''The function unzips the downloaded archive into the working directory'''\n",
    "\n",
    "    # Импорт библиотек\n",
    "    import zipfile\n",
    "    import os\n",
    "\n",
    "    # Проверка существования файла\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"The file {zip_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Разархивирование архива\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Archive extracted to {extract_to}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"The file is a bad zip file and cannot be extracted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     zip_file_path = \"C:\\\\Users\\\\Allen\\\\Downloads\\\\archive.zip\"\n",
    "#     destination_directory = \"D:\\\\GeekBrains\\\\Data_engineer_Diploma_project\"\n",
    "#     unzip_and_replace_dataset(zip_file_path, destination_directory)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование датасетов \n",
    "def transforming_datasets(test_path: str = \"D:\\\\GeekBrains\\\\Data_engineer_Diploma_project\\\\test.dat\", \n",
    "                          train_path: str = \"D:\\\\GeekBrains\\\\Data_engineer_Diploma_project\\\\train.dat\", \n",
    "                          test_csv_path: str = \"D:\\\\GeekBrains\\\\Data_engineer_Diploma_project\\\\ma_test.csv\", \n",
    "                          train_csv_path: str = \"D:\\\\GeekBrains\\\\Data_engineer_Diploma_project\\\\ma_train.csv\") -> None:\n",
    "    '''The function opens downloaded files, generates datasets adapted to processing based on them, and saves new datasets in .csv format'''\n",
    "\n",
    "    # Импорт библиотек\n",
    "    import pandas as pd\n",
    "\n",
    "    # В архиве датасеты (тренировочный и тестовый) содержатся в формате .dat\n",
    "    # поэтому, нам нужно их переформатировать в датасеты, пригодные и удобные для дальнейшего использования\n",
    "    # Чтение файла .dat\n",
    "\n",
    "    df_test = pd.read_fwf(test_path, sep='\\t', header=None)\n",
    "    df_train = pd.read_fwf(train_path, sep='\\t', header=None)\n",
    "\n",
    "    # Датафрейм df_test имеет атипичную ненормализованную структуру - всего 101 столбец, все аннотации содержатся в первом столбце, \n",
    "    # остальные колонки пустые, поэтому нам нужно создать датафрейм только из первой колонки.\n",
    "    # Датафрейм df_train имеет схожую структуру - 101 столбец, первая колонка - классы заболеваний,\n",
    "    # все аннотации содержатся во втором столбце, остальные колонки пустые, \n",
    "    # поэтому нам нужно создать датафрейм из первых двух столбцов.\n",
    "\n",
    "    # Трансформируем df_test в датасет формата .csv:\n",
    "    # Выбор только первого столбца\n",
    "    first = df_test.iloc[:, 0]\n",
    "\n",
    "    # Запись данных первого столбца в файл .csv с заголовком\n",
    "    first.to_csv(test_csv_path, index=False, header=['abstracts'])\n",
    "\n",
    "    # Теперь преобразуем df_train:\n",
    "    # Выбор первых двух сколонок\n",
    "    first_two = df_train.iloc[:, :2]\n",
    "\n",
    "    # Запись данных столбцов в файл .csv с заголовками\n",
    "    first_two.to_csv(train_csv_path, index=False, header=['labels', 'abstracts'])\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_dat_path = \"D:\\\\GeekBrains\\\\Data_engineer_Diploma_project\\\\test.dat\"\n",
    "#     train_dat_path = \"D:\\\\GeekBrains\\\\Data_engineer_Diploma_project\\\\train.dat\"\n",
    "#     test_csv_path = \"D:\\\\GeekBrains\\\\Data_engineer_Diploma_project\\\\ma_test.csv\"\n",
    "#     train_csv_path = \"D:\\\\GeekBrains\\\\Data_engineer_Diploma_project\\\\ma_train.csv\"\n",
    "#     transforming_datasets(test_dat_path, train_dat_path, test_csv_path, train_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_to_labeling(input_csv: str = 'ma_train.csv', manual_label_csv: str = 'manual_label_sample.csv', \n",
    "                        rule_based_csv: str = 'rule_based_sample.csv', train_size: float = 0.01):\n",
    "    \"\"\"\n",
    "    Divides the dataframe into two parts for manual markup and for automatic rule-based markup.\n",
    "    \n",
    "    Parameters:\n",
    "    input_csv (str): Путь к исходному файлу CSV.\n",
    "    manual_label_csv (str): Путь к файлу CSV для ручной разметки.\n",
    "    rule_based_csv (str): Путь к файлу CSV для разметки на основе правил.\n",
    "    train_size (float): Доля датафрейма для ручной разметки.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Импорт библиотек\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split  \n",
    "\n",
    "    # Создание датафрейма Pandas из файла .csv\n",
    "    df_train = pd.read_csv(input_csv, engine='python', encoding='utf-8', on_bad_lines='skip', encoding_errors='ignore')\n",
    "    # Разделение датафрейма на две части - для ручной разметки и для разметки на основе правил\n",
    "    manual_label_sample, rule_based_sample = train_test_split(df_train, train_size=train_size, random_state=42)\n",
    "\n",
    "    # Сохранение датафреймов в файлы .csv для дальнейшей обработки\n",
    "    manual_label_sample.to_csv(manual_label_csv, index=False)\n",
    "    rule_based_sample.to_csv(rule_based_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split                # разделение данных на обучающую и тестовую части\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer         # преобразование текста в вектор\n",
    "from sklearn.linear_model import LogisticRegression                 # использование модели логистической регрессии\n",
    "from sklearn.metrics import accuracy_score, classification_report   # оценка производительности модели\n",
    "from sklearn.pipeline import Pipeline                               # конвеер обработки данных\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проводим разметку на основе правил.\n",
    "def rule_for_labeling(text: str) -> str:\n",
    "    '''The function defines a rule for assigning a label to the text and performs markup'''\n",
    "    \n",
    "    # Определяем списки с ключевыми значениями по каждой из четырех категорий \n",
    "    neoplasms_list = [\n",
    "        'neoplas', 'tumor', 'cancer', 'lymphom', 'blastoma', 'malign', 'benign', 'melanom', 'leukemi', 'metasta', 'carcinom', 'sarcoma', 'glioma',\n",
    "        'adenoma', 'chemotherapy', 'radiotherapy', 'oncology', 'carcinogenesis', 'mutagen', 'angiogenesis', 'radiation', 'immunotherapy', 'biopsy',\n",
    "        'brachytherapy', 'metastasis', 'prognosis', 'biological therapy', 'carcinoma', 'myeloma', 'genomics', 'immunology', 'cell stress',\n",
    "        'oncogene', 'tumorigenesis', 'cytology', 'histology', 'oncologist', 'neoplasm', 'oncogenic', 'tumor suppressor genes', 'malignancy',\n",
    "        'cancerous', 'non-cancerous', 'solid tumor', 'liquid tumor', 'tumor marker', 'oncogenesis', 'tumor microenvironment', 'carcinogenesis', \n",
    "        'adenocarcinoma', 'squamous cell carcinoma'\n",
    "    ]\n",
    "\n",
    "    digestive_list = [\n",
    "        'digestive', 'esophag', 'stomach', 'gastr', 'liver', 'cirrhosis', 'hepati', 'pancrea', 'intestin', 'sigmo', 'recto', 'rectu', 'cholecyst', \n",
    "        'gallbladder', 'portal pressure', 'portal hypertension', 'appendic', 'ulcer', 'bowel', 'dyspepsia', 'colitis', 'enteritis', 'gastroenteritis', \n",
    "        'endoscopy', 'colonoscopy', 'peptic', 'gastrointestinal', 'abdominal', 'dysphagia', 'diverticulitis', 'irritable bowel syndrome', \n",
    "        'inflammatory bowel disease', 'gastroesophageal reflux', 'celiac disease', 'crohn\\'s disease', 'ulcerative colitis',\n",
    "        'gastroscopy', 'biliary', 'esophageal', 'gastritis', 'hepatic', 'lactose intolerance', 'gastroenterologist', 'digestion', 'absorption', \n",
    "        'malabsorption', 'intestinal flora', 'microbiota', 'probiotics', 'prebiotics', 'dietary fiber', 'nutrition'\n",
    "    ]\n",
    "\n",
    "    neuro_list = [\n",
    "        'neuro', 'nerv', 'reflex', 'brain', 'cerebr', 'white matter', 'subcort', 'plegi', 'intrathec', 'medulla', 'mening', 'epilepsy', \n",
    "        'multiple sclerosis', 'parkinson\\'s disease', 'alzheimer\\'s disease', 'seizure', 'paresthesia', 'dementia', 'encephalopathy', \n",
    "        'neuropathy', 'neurodegeneration', 'stroke', 'cerebral', 'spinal cord', 'neurotransmitter', 'synapse', 'neuralgia', 'neurology', \n",
    "        'neurosurgery', 'neurooncology', 'neurovascular', 'autonomic nervous system', 'central nervous system', 'peripheral nervous system', \n",
    "        'brain injury', 'concussion', 'traumatic brain injury', 'spinal injury', 'neurological disorder', 'neurodevelopmental disorders',\n",
    "        'neurodegenerative disorders', 'neuroinflammation', 'neuroimaging', 'neuroscience', 'neurophysiology', 'neurotransmission', \n",
    "        'neuroplasticity', 'neurogenesis', 'neuroendocrinology', 'neuropsychology', 'neurotoxicity', 'neuromodulation', 'neuroprotection', \n",
    "        'neuropathology'\n",
    "    ]\n",
    "\n",
    "    cardio_list = [\n",
    "        'cardi', 'heart', 'vascul', 'embolism', 'stroke', 'reperfus', 'thromboly', 'ischemi', 'hypercholesterolemia', 'hyperten', 'blood pressure', \n",
    "        'valv', 'ventric', 'aneurysm', 'coronar', 'arter', 'aort', 'electrocardiogra', 'arrhythm', 'clot', 'mitral', 'endocard', 'hypertension', \n",
    "        'myocardial', 'infarction', 'cardiover', 'fibrillat', 'bypass', 'pericarditis', 'cardiomyopathy', 'hypotension', 'angiography', 'stenting', \n",
    "        'cardiac catheterization', 'vascular', 'echocardiogram', 'cardiogenic', 'angioplasty', 'cardiac arrest', 'heart failure', \n",
    "        'cardiac rehabilitation', 'electrophysiology', 'heart valve disease', 'cardiopulmonary', 'cardiothoracic surgery', 'vascular surgery', \n",
    "        'cardiovascular disease', 'cardiovascular health', 'cardiovascular risk', 'cardiovascular system', 'cardioprotection', 'cardiovascular imaging', \n",
    "        'cardiovascular physiology', 'cardiovascular pharmacology', 'cardiovascular intervention', 'cardiovascular diagnostics', 'cardiovascular genetics'\n",
    "    ]\n",
    "\n",
    "    # Приведем текст аннотаций к нижнему регистру\n",
    "    row = text.lower()\n",
    "    \n",
    "    # В используемом датасете используется следующая маркировка:\n",
    "    # neoplasms = 1\n",
    "    # digestive system diseases = 2\n",
    "    # nervous system diseases = 3\n",
    "    # cardiovascular diseases = 4\n",
    "    # general pathological conditions = 5\n",
    "\n",
    "    # Создаём словарь в котором ключи - категории заболеваний, а значения - количество ключевых значений в тексте по каждой категории\n",
    "    res_dict = {\n",
    "        '1': 0,\n",
    "        '2': 0,\n",
    "        '3': 0,\n",
    "        '4': 0\n",
    "    }\n",
    "    # Рассчитываем количество ключевых значений в тексте и заполняем словарь\n",
    "    for p in neoplasms_list:\n",
    "        res_dict['1'] += row.count(p)\n",
    "    for d in digestive_list:\n",
    "        res_dict['2'] += row.count(d)\n",
    "    for n in neuro_list:\n",
    "        res_dict['3'] += row.count(n)\n",
    "    for c in cardio_list:\n",
    "        res_dict['4'] += row.count(c)\n",
    "    \n",
    "    # Рассчитываем наиболее часто встречаемую категорию в тексте и её отношение ко всем выявленным значения по всем категориям.\n",
    "    # Для отнесения текста к определенной категории его доля должна превышать условно взятое значение - 0,3.\n",
    "    # Если не превышает, то текст будет отнесён к категории 'general pathological conditions' и ему будет присвоена метка - 5\n",
    "    most_frequent = max(res_dict.values())\n",
    "    divisor = sum(res_dict.values())\n",
    "    if divisor > 0 and (most_frequent / divisor) > 0.3:\n",
    "        for key, value in res_dict.items(): \n",
    "            if value == most_frequent:\n",
    "                return int(key)\n",
    "    else:\n",
    "        return int(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_labeling() -> pd.DataFrame:\n",
    "\n",
    "    # Импорт библиотеки\n",
    "    import pandas as pd\n",
    "    from text_clas_pkg import rule_for_labeling\n",
    "\n",
    "    # Создание датафрейма Pandas\n",
    "    df_rule = pd.read_csv('rule_based_sample.csv')\n",
    "\n",
    "    # Разметка датафрейма - добавляем в датафрейм колонку в которой \n",
    "    # будут метки на онове определенного нами правила\n",
    "    df_rule['labeled condition name'] = df_rule['medical_abstract'].apply(rule_for_labeling)\n",
    "\n",
    "    return df_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ручную разметку выборки выдержек из медицинских статей в размере 144 шт (0,01 от всего датасета) я провел с использованием Label Studio. \n",
    "# Результат разметки сохранен в текущую директорию с именем ls_manual_labeled.csv.\n",
    "\n",
    "# Далее необходимо объединить два размеченных датасета. Для этого загрузим размеченный вручную датасет и посмотрим его структуру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merging_labeled_dfs(df_rule: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''The function combines the date frames obtained as a result of automatic \n",
    "    rule-based markup and manual markup and brings the combined dataframe to the \n",
    "    form in which it will be used to train the model'''\n",
    "    \n",
    "    # Импорт библиотек\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    # Проверка наличия датасета размеченного вручную в текущей директории\n",
    "    # Поскольку процесс будет выполняться автоматизированно, этап ручной разметки может быть исключен из процесса,\n",
    "    # либо выполняться не при каждом запуске процесса\n",
    "    dset_name = 'ls_manual_labeled.csv'  \n",
    "    dset_exists = os.path.exists(dset_name)\n",
    "\n",
    "    if dset_exists:\n",
    "        df_manual = pd.read_csv('ls_manual_labeled.csv')\n",
    "        # Для объединения датасетов приведем датасет созданный Label Studio к соответствующему виду:\n",
    "        df_manual.drop(['annotation_id', 'annotator', 'created_at', 'id', 'lead_time', 'updated_at'], axis=1, inplace=True)\n",
    "        df_manual.rename(columns={'sentiment': 'labeled condition name'}, inplace=True)\n",
    "        \n",
    "    # Теперь объединим датасеты:\n",
    "    df_merged = pd.concat([df_rule, df_manual])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для удобства дальнейшей работы удалим колонку condition_label, а так же добавим числовые значения, соответствующие размеченным нами данным.\n",
    "\n",
    "df_merged.drop(['condition_label'], axis=1, inplace=True)\n",
    "label_map = {\"neoplasms\": 1, \"digestive system diseases\": 2, \"nervous system diseases\": 3, \"cardiovascular diseases\": 4, \"general pathological conditions\": 5}\n",
    "df_merged['new condition label'] = df_common['labeled condition name'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переходим к обучению модели.\n",
    "# Для начала, ещё раз перемешаем датасет.\n",
    "\n",
    "df_common = shuffle(df_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим датасет на 2 части.\n",
    "\n",
    "labeled, unlabeled = train_test_split(df_common, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим функцию для обучения модели\n",
    "\n",
    "def train_model(labeled):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    x = vectorizer.fit_transform(labeled['medical_abstract'])\n",
    "    y = labeled['new condition label']\n",
    "\n",
    "    model = LogisticRegression(max_iter=12000)\n",
    "    model.fit(x, y)\n",
    "\n",
    "    return model, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели на выборке из размеченного датасета\n",
    "\n",
    "model, vectorizer = train_model(labeled=labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Использование модели на \"неразмеченной\" выборке\n",
    "\n",
    "\n",
    "x_unlabeled = vectorizer.transform(unlabeled['medical_abstract'])\n",
    "y_unlabeled_predicted = model.predict(x_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчёт энтропии предсказаний\n",
    "\n",
    "\n",
    "y_unlabeled_probe = model.predict_proba(x_unlabeled)\n",
    "uncertainty = -(y_unlabeled_probe * np.log2(y_unlabeled_probe)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбор 100 наиболее неопределенных точек (для маркировки человеком)\n",
    "\n",
    "\n",
    "labeled_new = unlabeled.iloc[uncertainty.argsort()[:100]]\n",
    "unlabeled_new = unlabeled.iloc[uncertainty.argsort()[100:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разметка новых точек и добавление их к размеченному датасету\n",
    "\n",
    "\n",
    "labeled = pd.concat([labeled, labeled_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переобучение модели на расширенном размеченном множестве\n",
    "\n",
    "model, vectorizer = train_model(labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для оценки эффективности загрузим тестовый датасет\n",
    "\n",
    "test_df = pd.read_csv('medical_tc_test.csv', engine='python', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запустим модель\n",
    "\n",
    "x_test = vectorizer.transform(test_df['medical_abstract'])\n",
    "y_test_predicted = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценим эффективность\n",
    "\n",
    "f1 = f1_score(test_df['condition_label'], y_test_predicted, average='micro')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вариант 2\n",
    "\n",
    "# На объединенном размеченном датасете:\n",
    "X = df_common['medical_abstract']\n",
    "Y = df_common['new condition label']\n",
    "\n",
    "vectorizer2 = TfidfVectorizer()\n",
    "X_vectorized = vectorizer2.fit_transform(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_vectorized, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "model2 = LogisticRegression(max_iter=15000)\n",
    "model2.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "Y_test_predicted = model2.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(Y_test, Y_test_predicted)\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "# И ещё раз на тестовом датасете\n",
    "X2 = test_df['medical_abstract']\n",
    "Y2 = test_df['condition_label']\n",
    "\n",
    "vectorizer3 = TfidfVectorizer()\n",
    "X2_vectorized = vectorizer3.fit_transform(X2)\n",
    "\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2_vectorized, Y2, test_size=0.3, random_state=42)\n",
    "\n",
    "model3 = LogisticRegression(max_iter=15000)\n",
    "model3.fit(X2_train, Y2_train)\n",
    "\n",
    "\n",
    "Y2_test_predicted = model3.predict(X2_test)\n",
    "\n",
    "accuracy = accuracy_score(Y_test, Y_test_predicted)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
