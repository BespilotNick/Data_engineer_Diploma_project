{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета\n",
    "def getting_datasets() -> None:\n",
    "    '''The result of executing this function is a dataset downloaded into the directory \"Downloads\"'''\n",
    "\n",
    "    # Инициализируем WebDriver:\n",
    "    USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 YaBrowser/24.1.0.0 Safari/537.36'\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(f'user-agent={USER_AGENT}')\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    main_url = 'https://www.kaggle.com'\n",
    "    sign_in_url = 'https://www.kaggle.com/account/login'\n",
    "    # Датасет находится на странице: https://www.kaggle.com/datasets/chaitanyakck/medical-text/data\n",
    "    dataset_url = \"https://www.kaggle.com/datasets/chaitanyakck/medical-text/data\"\n",
    "\n",
    "    try:\n",
    "        # Перейдём на страницу входа в аккаунт Kaggle для авторизированного скачивания датасета\n",
    "        driver.get(sign_in_url)\n",
    "\n",
    "        # Нажмём на кнопку \"Sign in with Google\" для авторизации (подразумевая, что аккаунт уже зарегистрирован)\n",
    "        google_sign_in_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//button[contains(., \"Sign in with Google\")]'))\n",
    "            )        \n",
    "        google_sign_in_button.click()\n",
    "\n",
    "        # Добавим ожидание с целью снижения нагрузки на сервер.\n",
    "        time.sleep(3)\n",
    "\n",
    "        try:\n",
    "            # Проверим, что мы вошли в аккаунт и можем скачать датасет под своим аккаунтом.\n",
    "            if driver.find_element((By.XPATH, '//h1[contains(., \"Welcome\")]')):\n",
    "\n",
    "                # Теперь мы вошли в систему и можем переходить к дальнейшим действиям.\n",
    "                # Откроем веб-страницу с датасетом:\n",
    "                driver.get(dataset_url)\n",
    "\n",
    "                # Найдём кнопку \"Download\":\n",
    "                download_button = driver.find_element(By.XPATH, '//button[contains(., \"file_download\")]')\n",
    "                # Если кнопка найдена, нажмём на неё\n",
    "                if download_button:\n",
    "                    download_button.click()\n",
    "                # В противном случае воспользуемся альтернативным способом получения датасета - \n",
    "                # непосредственным ереходом по ссылке загрузки датасета\n",
    "                else:\n",
    "                    # найдём элемент, в котором содержится относительная ссылка\n",
    "                    href_element = driver.find_element(By.XPATH, '//div[@class=\"sc-emfenM sc-fnpAPw cvuSKw gzjyQr\"]/a')\n",
    "                    # извлечём относительную ссылку\n",
    "                    rel_link = href_element.get_attribute('href')\n",
    "                    # составим абсолютный путь на скачивание архива\n",
    "                    ds_download_link = urljoin(main_url, rel_link)\n",
    "                    # перейдём по прямой ссылке загрузки\n",
    "                    driver.get(ds_download_link)\n",
    "\n",
    "\n",
    "                # Можно использовать аналогичный код:\n",
    "                # В этом варианте кода заменён time.sleep() на явные ожидания WebDriverWait(), которые, возможно, являются более надежными.\n",
    "                \n",
    "                # try:\n",
    "                #     download_button = WebDriverWait(driver, 10).until(\n",
    "                #         EC.element_to_be_clickable((By.XPATH, '//button[contains(., \"file_download\")]'))\n",
    "                #     )\n",
    "                #     download_button.click()\n",
    "                # except Exception as e:\n",
    "                #     href_element = WebDriverWait(driver, 10).until(\n",
    "                #         EC.presence_of_element_located((By.XPATH, '//div[@class=\"sc-emfenM sc-fnpAPw cvuSKw gzjyQr\"]/a'))\n",
    "                #     )\n",
    "                #     rel_link = href_element.get_attribute('href')\n",
    "                #     ds_download_link = urljoin(main_url, rel_link)\n",
    "                #     driver.get(ds_download_link)\n",
    "\n",
    "                # WebDriverWait(driver, 10).until(\n",
    "                #     EC.presence_of_element_located((By.CLASS_NAME, \"download-modal\"))\n",
    "                # )\n",
    "\n",
    "                # Также, можно добавить контекстный менеджер with для инициализации и автоматического закрытия WebDriver после завершения работы функции.\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Произошла ошибка в процессе поиска элемента на странице - {e}')\n",
    "        # Подождём, пока загрузится файл:\n",
    "        # Используем ожидание появления элемента с определенным классом, указывающим на загрузку\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"download-modal\"))\n",
    "        )\n",
    "    except Exception as E:\n",
    "        print(f'Произошла ошибка в процессе авторизации - {E}')\n",
    "    # Закроем браузер в любом случае\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "getting_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_dataset_by_api(ds_name: str = 'chaitanyakck/medical-text', path: str = os.getcwd(), unzip: bool = True) -> None:\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_download_files(ds_name, path, unzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разархивирование загруженного архива в рабочую директорию\n",
    "def unzip_and_replace_datasets(zip_path: str =\"C:\\\\Users\\\\Allen\\\\Downloads\\\\archive.zip\", \n",
    "                              extract_to: str = os.getcwd()) -> None:\n",
    "    '''The function unzips the downloaded archive into the working directory'''\n",
    "\n",
    "    # Проверка существования файла\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"The file {zip_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Разархивирование архива\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Archive extracted to {extract_to}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"The file is a bad zip file and cannot be extracted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование датасетов \n",
    "def transforming_datasets(test_path: str = \"test.dat\", train_path: str = \"train.dat\", \n",
    "                          test_csv_path: str = \"ma_test.csv\", train_csv_path: str = \"ma_train.csv\") -> pd.DataFrame:\n",
    "    '''The function opens downloaded files, generates datasets adapted to processing based on them, and saves new datasets in .csv format'''\n",
    "\n",
    "    # В архиве датасеты (тренировочный и тестовый) содержатся в формате .dat\n",
    "    # поэтому, нам нужно их переформатировать в датасеты, пригодные и удобные для дальнейшего использования\n",
    "    # Чтение файла .dat\n",
    "\n",
    "    df_test = pd.read_fwf(test_path, sep='\\t', header=None)\n",
    "    df_train = pd.read_fwf(train_path, sep='\\t', header=None)\n",
    "\n",
    "    # Датафрейм df_test имеет атипичную ненормализованную структуру - всего 101 столбец, все аннотации содержатся в первом столбце, \n",
    "    # остальные колонки пустые, поэтому нам нужно создать датафрейм только из первой колонки.\n",
    "    # Датафрейм df_train имеет схожую структуру - 101 столбец, первая колонка - классы заболеваний,\n",
    "    # все аннотации содержатся во втором столбце, остальные колонки пустые.\n",
    "    # Для нашей дальнейшей работы метки классов нам не требуются,\n",
    "    # поэтому нам нужно создать датафрейм из второго столбца.\n",
    "\n",
    "    # Трансформируем df_test в датасет формата .csv:\n",
    "    # Выбор только первого столбца\n",
    "    df_ma_test = df_test.iloc[:, [0]].rename(columns={0: 'abstracts'})\n",
    "\n",
    "    # Запись данных первого столбца в файл .csv с заголовком\n",
    "    df_ma_test.to_csv(test_csv_path, index=False, header=['abstracts'])\n",
    "\n",
    "    # Теперь преобразуем df_train:\n",
    "    # Выбор второго столбца\n",
    "    df_ma_train = df_train.iloc[:, [1]].rename(columns={1: 'abstracts'})\n",
    "\n",
    "    # Запись данных столбцов в файл .csv с заголовком\n",
    "    df_ma_train.to_csv(train_csv_path, index=False, header=['abstracts'])\n",
    "\n",
    "    return df_ma_train, df_ma_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dfs_to_labeling(df_train: pd.DataFrame, manual_label_csv: str = 'manual_label_sample.csv', \n",
    "                        rule_based_csv: str = 'rule_based_sample.csv', train_size: float = 0.01):\n",
    "    \"\"\"\n",
    "    Divides the dataframe into two parts for manual markup and for automatic rule-based markup.\n",
    "    Returns dataframe for automatic rule-based markup.\n",
    "    \n",
    "    Parameters:\n",
    "    df_train: pd.DataFrame: Датафрейм получаемый из функции 'transforming_datasets'.\n",
    "    manual_label_csv (str): Путь к файлу CSV для ручной разметки.\n",
    "    rule_based_csv (str): Путь к файлу CSV для разметки на основе правил.\n",
    "    train_size (float): Доля датафрейма для ручной разметки.\n",
    "    \"\"\"\n",
    "\n",
    "    # Разделение датафрейма на две части - для ручной разметки и для разметки на основе правил\n",
    "    manual_label_sample, rule_based_sample = train_test_split(df_train, train_size=train_size, random_state=42)\n",
    "\n",
    "    # Сохранение датафреймов в файлы .csv для дальнейшей обработки\n",
    "    manual_label_sample.to_csv(manual_label_csv, index=False)\n",
    "    rule_based_sample.to_csv(rule_based_csv, index=False)\n",
    "\n",
    "    return rule_based_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_for_labeling(text: str) -> int:\n",
    "    '''The function defines a rule for assigning a label to the text and performs markup'''\n",
    "    \n",
    "    # Определяем списки с ключевыми значениями по каждой из четырех категорий \n",
    "    neoplasms_list = [\n",
    "        'neoplas', 'tumor', 'cancer', 'lymphom', 'blastoma', 'malign', 'benign', 'melanom', 'leukemi', 'metasta', 'carcinom', 'sarcoma', 'glioma',\n",
    "        'adenoma', 'chemotherapy', 'radiotherapy', 'oncology', 'carcinogenesis', 'mutagen', 'angiogenesis', 'radiation', 'immunotherapy', 'biopsy',\n",
    "        'brachytherapy', 'metastasis', 'prognosis', 'biological therapy', 'carcinoma', 'myeloma', 'genomics', 'immunology', 'cell stress',\n",
    "        'oncogene', 'tumorigenesis', 'cytology', 'histology', 'oncologist', 'neoplasm', 'oncogenic', 'tumor suppressor genes', 'malignancy',\n",
    "        'cancerous', 'non-cancerous', 'solid tumor', 'liquid tumor', 'tumor marker', 'oncogenesis', 'tumor microenvironment', 'carcinogenesis', \n",
    "        'adenocarcinoma', 'squamous cell carcinoma'\n",
    "    ]\n",
    "\n",
    "    digestive_list = [\n",
    "        'digestive', 'esophag', 'stomach', 'gastr', 'liver', 'cirrhosis', 'hepati', 'pancrea', 'intestin', 'sigmo', 'recto', 'rectu', 'cholecyst', \n",
    "        'gallbladder', 'portal pressure', 'portal hypertension', 'appendic', 'ulcer', 'bowel', 'dyspepsia', 'colitis', 'enteritis', 'gastroenteritis', \n",
    "        'endoscopy', 'colonoscopy', 'peptic', 'gastrointestinal', 'abdominal', 'dysphagia', 'diverticulitis', 'irritable bowel syndrome', \n",
    "        'inflammatory bowel disease', 'gastroesophageal reflux', 'celiac disease', 'crohn\\'s disease', 'ulcerative colitis',\n",
    "        'gastroscopy', 'biliary', 'esophageal', 'gastritis', 'hepatic', 'lactose intolerance', 'gastroenterologist', 'digestion', 'absorption', \n",
    "        'malabsorption', 'intestinal flora', 'microbiota', 'probiotics', 'prebiotics', 'dietary fiber', 'nutrition'\n",
    "    ]\n",
    "\n",
    "    neuro_list = [\n",
    "        'neuro', 'nerv', 'reflex', 'brain', 'cerebr', 'white matter', 'subcort', 'plegi', 'intrathec', 'medulla', 'mening', 'epilepsy', \n",
    "        'multiple sclerosis', 'parkinson\\'s disease', 'alzheimer\\'s disease', 'seizure', 'paresthesia', 'dementia', 'encephalopathy', \n",
    "        'neuropathy', 'neurodegeneration', 'stroke', 'cerebral', 'spinal cord', 'neurotransmitter', 'synapse', 'neuralgia', 'neurology', \n",
    "        'neurosurgery', 'neurooncology', 'neurovascular', 'autonomic nervous system', 'central nervous system', 'peripheral nervous system', \n",
    "        'brain injury', 'concussion', 'traumatic brain injury', 'spinal injury', 'neurological disorder', 'neurodevelopmental disorders',\n",
    "        'neurodegenerative disorders', 'neuroinflammation', 'neuroimaging', 'neuroscience', 'neurophysiology', 'neurotransmission', \n",
    "        'neuroplasticity', 'neurogenesis', 'neuroendocrinology', 'neuropsychology', 'neurotoxicity', 'neuromodulation', 'neuroprotection', \n",
    "        'neuropathology'\n",
    "    ]\n",
    "\n",
    "    cardio_list = [\n",
    "        'cardi', 'heart', 'vascul', 'embolism', 'stroke', 'reperfus', 'thromboly', 'ischemi', 'hypercholesterolemia', 'hyperten', 'blood pressure', \n",
    "        'valv', 'ventric', 'aneurysm', 'coronar', 'arter', 'aort', 'electrocardiogra', 'arrhythm', 'clot', 'mitral', 'endocard', 'hypertension', \n",
    "        'myocardial', 'infarction', 'cardiover', 'fibrillat', 'bypass', 'pericarditis', 'cardiomyopathy', 'hypotension', 'angiography', 'stenting', \n",
    "        'cardiac catheterization', 'vascular', 'echocardiogram', 'cardiogenic', 'angioplasty', 'cardiac arrest', 'heart failure', \n",
    "        'cardiac rehabilitation', 'electrophysiology', 'heart valve disease', 'cardiopulmonary', 'cardiothoracic surgery', 'vascular surgery', \n",
    "        'cardiovascular disease', 'cardiovascular health', 'cardiovascular risk', 'cardiovascular system', 'cardioprotection', 'cardiovascular imaging', \n",
    "        'cardiovascular physiology', 'cardiovascular pharmacology', 'cardiovascular intervention', 'cardiovascular diagnostics', 'cardiovascular genetics'\n",
    "    ]\n",
    "\n",
    "    # Приведем текст аннотаций к нижнему регистру\n",
    "    row = text.lower()\n",
    "    \n",
    "    # В используемом датасете используется следующая маркировка:\n",
    "    # neoplasms = 1\n",
    "    # digestive system diseases = 2\n",
    "    # nervous system diseases = 3\n",
    "    # cardiovascular diseases = 4\n",
    "    # general pathological conditions = 5\n",
    "\n",
    "    # Создаём словарь в котором ключи - категории заболеваний, а значения - количество ключевых значений в тексте по каждой категории\n",
    "    res_dict = {\n",
    "        '1': 0,\n",
    "        '2': 0,\n",
    "        '3': 0,\n",
    "        '4': 0\n",
    "    }\n",
    "    # Рассчитываем количество ключевых значений в тексте и заполняем словарь\n",
    "    for p in neoplasms_list:\n",
    "        res_dict['1'] += row.count(p)\n",
    "    for d in digestive_list:\n",
    "        res_dict['2'] += row.count(d)\n",
    "    for n in neuro_list:\n",
    "        res_dict['3'] += row.count(n)\n",
    "    for c in cardio_list:\n",
    "        res_dict['4'] += row.count(c)\n",
    "    \n",
    "    # Рассчитываем наиболее часто встречаемую категорию в тексте и её отношение ко всем выявленным значения по всем категориям.\n",
    "    # Для отнесения текста к определенной категории его доля должна превышать условно взятое значение - 0,3.\n",
    "    # Если не превышает, то текст будет отнесён к категории 'general pathological conditions' и ему будет присвоена метка - 5\n",
    "    most_frequent = max(res_dict.values())\n",
    "    divisor = sum(res_dict.values())\n",
    "    if divisor > 0 and (most_frequent / divisor) > 0.3:\n",
    "        for key, value in res_dict.items(): \n",
    "            if value == most_frequent:\n",
    "                return int(key)\n",
    "    else:\n",
    "        return int(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_labeling(df_rbs: pd.DataFrame):\n",
    "    '''The function performs the markup of the dataframe - we add a column to the dataframe, \n",
    "    in which there will be labels based on a rule defined by us'''\n",
    "\n",
    "    df_rbs['labeled_condition_mark'] = df_rbs['abstracts'].apply(rule_for_labeling)\n",
    "    return df_rbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ручную разметку выборки выдержек из медицинских статей в размере 144 шт (0,01 от всего датасета) я провел в Label Studio,\n",
    "# с использованием маркировки цифровыми значениями. \n",
    "# Результат разметки сохранен в текущую директорию с именем ls_manual_labeled.csv.\n",
    "\n",
    "# Далее необходимо объединить два размеченных датасета. Для этого загрузим размеченный вручную датасет и посмотрим его структуру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединение датасетов, если есть датасет, размеченный вручную, \n",
    "# и приведение их к виду который будет использоваться для обучения модели\n",
    "def merging_labeled_dfs(df_rule: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''The function combines the date frames obtained as a result of automatic \n",
    "    rule-based markup and manual markup and brings the combined dataframe to the \n",
    "    form in which it will be used to train the model'''\n",
    "\n",
    "    # Проверка наличия датасета размеченного вручную в текущей директории\n",
    "    # Поскольку процесс будет выполняться автоматизированно, этап ручной разметки может быть исключен из процесса,\n",
    "    # либо выполняться не при каждом запуске процесса\n",
    "    dset_name = 'ls_manual_labeled.csv'  \n",
    "    dset_exists = os.path.exists(dset_name)\n",
    "\n",
    "    if dset_exists:\n",
    "        df_manual = pd.read_csv('ls_manual_labeled.csv')\n",
    "        # Для объединения датасетов приведем датасет созданный Label Studio к соответствующему виду:\n",
    "        df_manual.drop(['annotation_id', 'annotator', 'created_at', 'id', 'lead_time', 'updated_at'], axis=1, inplace=True)\n",
    "        df_manual.rename(columns={'sentiment': 'labeled_condition_mark'}, inplace=True)\n",
    "        # Теперь объединим датасеты:\n",
    "        df_merged = pd.concat([df_rule, df_manual])\n",
    "    # Если датасета размеченного вручную в текущей директории нет,\n",
    "    # то итоговым датафреймом будет датафрейм, размеченный на основе правила\n",
    "    else:\n",
    "        df_merged = df_rule\n",
    "\n",
    "    # Сохраним результирующий датасет\n",
    "    df_merged.to_csv('merged_dataset.csv', index=False)\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переходим к обучению модели.\n",
    "# На объединенном размеченном датасете:\n",
    "def teaching_and_saving_model(train_df: pd.DataFrame):\n",
    "    '''The function trains a machine learning model on a marked-up dataset, saves the model and \n",
    "    a vectorizer for further use, and returns a dataframe with the markup'''\n",
    "\n",
    "    # Для начала, перемешаем датасет.\n",
    "    train_df = shuffle(train_df)\n",
    "\n",
    "    X = train_df['abstracts']\n",
    "    Y = train_df['labeled_condition_mark']\n",
    "\n",
    "    # Создаем векторизатор и преобразуем тексты в векторы\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "    # Обучаем модель\n",
    "    model = LogisticRegression(max_iter=15000)\n",
    "    model.fit(X_vectorized, Y)\n",
    "\n",
    "    # Делаем предсказания на всех данных\n",
    "    Y_predicted = model.predict(X_vectorized)\n",
    "\n",
    "    # Добавление колонки с предсказанными значениями в датафрейм\n",
    "    train_df['predicted_mark'] = Y_predicted\n",
    "\n",
    "    # Сохранение датасета с предсказанными значениями\n",
    "    train_df.to_csv('ma_train_with_predictions.csv', index=False)\n",
    "\n",
    "    # Сохранение модели и векторизатора\n",
    "    dump(model, 'model_ma_trained.joblib')\n",
    "    dump(vectorizer, 'vectorizer_ma_trained.joblib')\n",
    "\n",
    "    return train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(test_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''The function loads a trained machine learning model and applies it to an untagged dataframe'''\n",
    "    # Загрузка модели\n",
    "    model = load('model_ma_trained.joblib')\n",
    "    # Загрузка векторизатора\n",
    "    vectorizer = load('vectorizer_ma_trained.joblib')\n",
    "\n",
    "    # Преобразование текстовых данных нового датафрейма в векторный формат\n",
    "    X_new = vectorizer.transform(test_df['abstracts'])\n",
    "    # Используем модель для предсказания меток новых данных\n",
    "    Y_new_predicted = model.predict(X_new)\n",
    "\n",
    "    # разметка тестового датасета на основе правил для послдующей\n",
    "    # оценки эффективности модели\n",
    "    test_df = rule_based_labeling(test_df)\n",
    "\n",
    "    # Добавление колонки с предсказанными значениями в датафрейм\n",
    "    test_df['predicted_mark'] = Y_new_predicted\n",
    "   \n",
    "    # Сохранение датасета с размеченными и предсказанными значениями\n",
    "    test_df.to_csv('ma_test_with_predictions.csv', index=False)\n",
    "\n",
    "    return test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценим эффективность модели.\n",
    "def accuracy_scoring(df_for_evaluation: pd.DataFrame):\n",
    "    '''The function evaluates the effectiveness of the machine learning model \n",
    "    and saves results into .txt files'''\n",
    "    \n",
    "    true_labels = df_for_evaluation['labeled_condition_mark']\n",
    "    predicted_labels = df_for_evaluation['predicted_mark']\n",
    "    \n",
    "    # Вычисление точности\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    # Вычисление других метрик\n",
    "    report = classification_report(true_labels, predicted_labels)\n",
    "    # Построение матрицы ошибок\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Получение текущей даты и времени\n",
    "    current_time = datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Сохранение точности с датой и временем\n",
    "    with open('accuracy.txt', 'a') as f:\n",
    "        f.write(f\"Accuracy: {accuracy} (Дата и время: {formatted_time})\\n\")\n",
    "\n",
    "    # Сохранение отчета о классификации с датой и временем\n",
    "    with open('classification_report.txt', 'a') as f:\n",
    "        f.write(f\"Отчет о классификации (Дата и время: {formatted_time}):\\n{report}\\n\")\n",
    "\n",
    "    # Сохранение матрицы ошибок с датой и временем\n",
    "    with open('confusion_matrix.txt', 'a') as f:\n",
    "        f.write(f\"\\nМатрица ошибок (Дата и время: {formatted_time}):\\n\")\n",
    "        for line in conf_matrix:\n",
    "            f.write(' '.join(str(x) for x in line) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(mysql_conn_id: str, database_name: str):\n",
    "    # Подключение к MySQL\n",
    "    import MySQLdb\n",
    "    from airflow.hooks.base_hook import BaseHook\n",
    "    # Получение параметров подключения из Airflow\n",
    "    connection_params = BaseHook.get_connection(mysql_conn_id) \n",
    "    connection = MySQLdb.connect(\n",
    "        user=connection_params.login,\n",
    "        passwd=connection_params.password,\n",
    "        host=connection_params.host\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    # Создание базы данных\n",
    "    cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {database_name};\")\n",
    "    # Закрытие соединения\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(f\"Database {database_name} created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def write_dataframe_to_mysql(table_name: str, df: pd.DataFrame, mysql_conn_id: str):\n",
    "    # Получение параметров подключения из Airflow\n",
    "    connection_params = BaseHook.get_connection(mysql_conn_id)\n",
    "    conn_str = f\"mysql+mysqldb://{connection_params.login}:{connection_params.password}\" \\\n",
    "               f\"@{connection_params.host}/{connection_params.schema}\"\n",
    "    engine = create_engine(conn_str)\n",
    "\n",
    "    # Запись датафрейма в базу данных MySQL\n",
    "    df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "    print(f\"Dataframe is written to MySQL table {table_name} successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск всего процесса последовательно в коде Python\n",
    "if __name__ == \"__main__\":\n",
    "     \n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from urllib.parse import urljoin\n",
    "    from sklearn.model_selection import train_test_split                # разделение данных на обучающую и тестовую части\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer         # преобразование текста в вектор\n",
    "    from sklearn.linear_model import LogisticRegression                 # использование модели логистической регрессии\n",
    "    from sklearn.utils import shuffle\n",
    "    from joblib import dump, load\n",
    "    from datetime import datetime, timedelta\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    import time\n",
    "    import zipfile\n",
    "    import os\n",
    "    from sqlalchemy import create_engine\n",
    "    from airflow.models import Variable\n",
    "    from airflow.hooks.base_hook import BaseHook\n",
    "    import pandas as pd\n",
    "    \n",
    "    get_dataset_api(path=os.getcwd())\n",
    "    unzip_and_replace_datasets()\n",
    "\n",
    "    df_train, df_test = transforming_datasets()\n",
    "\n",
    "    df_prep = prepare_dfs_to_labeling(df_train)\n",
    "\n",
    "    df_rbl = rule_based_labeling(df_prep)\n",
    "    \n",
    "    df_merged = merging_labeled_dfs(df_rbl)\n",
    "\n",
    "    df_train_with_predictions = teaching_and_saving_model(df_merged)\n",
    "    accuracy_scoring(df_train_with_predictions)\n",
    "\n",
    "    df_test_with_predictions = testing_model(df_test)\n",
    "    accuracy_scoring(df_test_with_predictions)\n",
    "\n",
    "    create_database('airflow_db', 'DE_DP_text_classification')\n",
    "\n",
    "    write_dataframe_to_mysql(df_train_with_predictions)\n",
    "    write_dataframe_to_mysql(df_test_with_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
